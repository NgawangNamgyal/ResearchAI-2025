import requests
from bs4 import BeautifulSoup
import openai
import os
import fitz  # PyMuPDF
import re
import tkinter as tk
from tkinter import Toplevel, scrolledtext
from tkinter import ttk

# OPENAI_API_KEY is an env variable, if you need it, then contact me via my email namgylan@bxscience.edu





def is_new_link(link: str, used_links_file: str = "used_links.txt") -> bool:
    """Check if the link is new. If yes, add it to the file and return True."""
    if not os.path.exists(used_links_file):
        open(used_links_file, 'w').close()  # Create the file if it doesn't exist

    with open(used_links_file, 'r') as f:
        used_links = set(line.strip() for line in f)

    if link in used_links:
        return False  # Already used

    # New link, so add it
    with open(used_links_file, 'a') as f:
        f.write(link + "\n")
    return True


# --- Helper to download and parse arXiv PDF ---
def download_pdf_text(arxiv_url: str) -> str:
    pdf_url = arxiv_url.replace("/abs/", "/pdf/") + ".pdf"
    response = requests.get(pdf_url)
    response.raise_for_status()

    with open("temp.pdf", "wb") as f:
        f.write(response.content)

    with fitz.open("temp.pdf") as doc:
        return "\n".join(page.get_text() for page in doc)

# --- Extract section like Results or Discussion ---
def extract_results_section(text: str, section_keywords=["results", "discussion"], word_limit=100) -> str:
    lines = text.splitlines()
    section_text = ""
    recording = False

    for line in lines:
        lower = line.lower().strip()
        if any(kw in lower for kw in section_keywords):
            recording = True
        elif recording and (lower.startswith("references") or lower.startswith("acknowledg")):
            break

        if recording:
            section_text += " " + line.strip()

    # Limit to 100 words
    words = section_text.split()
    return " ".join(words[:word_limit]) if words else "Section not found."

# --- Find sentence containing "space group" and its neighbors ---

def extract_neighbor_sentences(text: str, target_phrases: list) -> str:
    """
    Extracts the sentence containing any target phrase, plus one before and one after.
    Prevents duplication of overlapping or repeated snippets.
    """
    # Split the text into sentences
    sentences = re.split(r'(?<=[.!?])\s+', text)
    results = []
    used_indices = set()

    for i, sentence in enumerate(sentences):
        # Check if any target phrase is in the current sentence
        if any(phrase in sentence.lower() for phrase in target_phrases):
            # Define the range of sentences to include: previous, current, next
            idx_range = set(range(max(i - 1, 0), min(i + 2, len(sentences))))

            # If any of these indices were already used, skip to avoid duplication
            if idx_range & used_indices:
                continue

            # Mark indices as used
            used_indices.update(idx_range)

            # Extract snippet and add to results
            snippet = " ".join(sentences[j] for j in sorted(idx_range)).strip()
            results.append(snippet)

    return "\n\n".join(results) if results else "No target phrases found."
# --- arXiv Search and Extraction ---
def get_arXiv_info(topic: str, num_papers: int = 5) -> str:
    print("[Step 1]: Getting Links from arXiv")
    search_url = (
        f"https://arxiv.org/search/?query={topic}"
        "&searchtype=all&abstracts=show&order=-announced_date_first&size=50"
    )
    resp = requests.get(search_url)
    resp.raise_for_status()
    doc = BeautifulSoup(resp.text, "html.parser")

    links = []
    for a in doc.find_all('a', href=True):
        href = a['href']
        if href.startswith('https://arxiv.org/abs/') and is_new_link(href):
            links.append(href)
            if len(links) >= num_papers:
                break
    links = list(dict.fromkeys(links))[:num_papers]

    abstracts = []
    for i, link in enumerate(links, start=1):
        print(f"[Paper {i}] Fetching abstract and PDF content from {link}")
        paper_resp = requests.get(link)
        paper_resp.raise_for_status()
        paper_soup = BeautifulSoup(paper_resp.text, "html.parser")

        meta_tag = paper_soup.find("meta", {"property": "og:description"})
        abstract = meta_tag["content"] if meta_tag and meta_tag.get("content") else "No abstract found."
        target_phrases = ["space group", "electronic properties", "elastic properties", "bulk modulus", "Young modulus", "Shear modulus", "Poissonâ€™s ratio", "tangent modulus", "chord modulus", "hardness", "electrical conductivity", "carrier concentration", "carrier mobility", "band gap", "electron effective mass", "dielectric constant" ,"permittivity" , "charge recombination and lifetime"]
        # PDF text extraction
        try:
            pdf_text = download_pdf_text(link)
            result_snippet = extract_results_section(pdf_text)
            properties = extract_neighbor_sentences(pdf_text, target_phrases)
        except Exception as e:
            result_snippet = f"Error reading PDF: {e}"
            properties = "N/A"

        abstracts.append(
            f"Publication #{i} ({link}):\n"
            f"Abstract:\n{abstract}\n\n"
            f"PDF Results Section (100 words max):\n{result_snippet}\n\n"
            f"Context Around 'Space Group':\n{properties}\n"
        )

    return "\n\n".join(abstracts)

# --- ChatGPT Answering Function ---
def ask_chatgpt(abstracts: str, topic: str, question: str) -> str:
    print("[Step 3] Sending to ChatGPT...")
    client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
    prompt = (
        f"I have collected the following abstracts and extracted content on '{topic}':\n\n"
        f"{abstracts}\n\n"
        f"Based on this, please answer the following question:\n"
        f"{question}\n\n"
        f"Respond with concise answer in the form of bullet points and cite publication numbers/links. "
        f"If the info is insufficient, say 'More Information Required'."
    )
    response = client.chat.completions.create(
        model="gpt-4.1-nano",
        messages=[
            {"role": "system", "content": "You are an expert assistant summarizing scientific papers."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.5,
        max_tokens=1500
    )
    return response.choices[0].message.content


# --- Main ---

def main():
    topic = topic_entry.get()
    abstracts = get_arXiv_info(topic)
    question = question_entry.get()
    answer = ask_chatgpt(abstracts, topic, question)
    print(answer)
    output_text.config(state="normal")  # Make the Text widget editable temporarily
    output_text.delete("1.0", tk.END)  # Clear previous content
    output_text.insert(tk.END, f"Response:\n {answer}")
    output_text.config(state="disabled")  # Make it read-only again


# Create main window
root = tk.Tk()
root.title("Simple GUI with Scrolling Output")
root.geometry("450x300")
root.attributes("-fullscreen", True)


# Topic label and entry
tk.Label(root, text="Enter topic:").pack(anchor='w', padx=10, pady=(10, 0))
topic_entry = tk.Entry(root, width=50)
topic_entry.pack(padx=10, pady=(0, 10))

# Question label and entry
tk.Label(root, text="Enter question:").pack(anchor='w', padx=10)
question_entry = tk.Entry(root, width=50)
question_entry.pack(padx=10, pady=(0, 10))

# Submit button
tk.Button(root, text="Submit", command=main).pack(pady=(0, 10))

# Text box with vertical scrollbar
frame = tk.Frame(root)
frame.pack(fill="both", expand=True, padx=10, pady=5)

output_text = tk.Text(frame, wrap="word", height=8, state="disabled")
output_text.pack(side="left", fill="both", expand=True)

scrollbar = ttk.Scrollbar(frame, orient="vertical", command=output_text.yview)
scrollbar.pack(side="right", fill="y")
output_text.config(yscrollcommand=scrollbar.set)

# Run GUI
root.mainloop()

