import requests
from bs4 import BeautifulSoup
import openai
import os
import fitz  # PyMuPDF
import re


#OPENAI_API_KEY is an env var. If you need it, contact me.

import tkinter as tk
from tkinter import messagebox
from tkinter import Toplevel, scrolledtext




def is_new_link(link: str, used_links_file: str = "used_links.txt") -> bool:
    """Check if the link is new. If yes, add it to the file and return True."""
    if not os.path.exists(used_links_file):
        open(used_links_file, 'w').close()  # Create the file if it doesn't exist

    with open(used_links_file, 'r') as f:
        used_links = set(line.strip() for line in f)

    if link in used_links:
        return False  # Already used

    # New link, so add it
    with open(used_links_file, 'a') as f:
        f.write(link + "\n")
    return True


# --- Helper to download and parse arXiv PDF ---
def download_pdf_text(arxiv_url: str) -> str:
    pdf_url = arxiv_url.replace("/abs/", "/pdf/") + ".pdf"
    response = requests.get(pdf_url)
    response.raise_for_status()

    with open("temp.pdf", "wb") as f:
        f.write(response.content)

    with fitz.open("temp.pdf") as doc:
        return "\n".join(page.get_text() for page in doc)

# --- Extract section like Results or Discussion ---
def extract_results_section(text: str, section_keywords=["results", "discussion"], word_limit=100) -> str:
    lines = text.splitlines()
    section_text = ""
    recording = False

    for line in lines:
        lower = line.lower().strip()
        if any(kw in lower for kw in section_keywords):
            recording = True
        elif recording and (lower.startswith("references") or lower.startswith("acknowledg")):
            break

        if recording:
            section_text += " " + line.strip()

    # Limit to 100 words
    words = section_text.split()
    return " ".join(words[:word_limit]) if words else "Section not found."

# --- Find sentence containing "space group" and its neighbors ---

def extract_neighbor_sentences(text: str, target_phrases: list) -> str:
    """
    Extracts the sentence containing any target phrase, plus one before and one after.
    Prevents duplication of overlapping or repeated snippets.
    """
    # Split the text into sentences
    sentences = re.split(r'(?<=[.!?])\s+', text)
    results = []
    used_indices = set()

    for i, sentence in enumerate(sentences):
        # Check if any target phrase is in the current sentence
        if any(phrase in sentence.lower() for phrase in target_phrases):
            # Define the range of sentences to include: previous, current, next
            idx_range = set(range(max(i - 1, 0), min(i + 2, len(sentences))))

            # If any of these indices were already used, skip to avoid duplication
            if idx_range & used_indices:
                continue

            # Mark indices as used
            used_indices.update(idx_range)

            # Extract snippet and add to results
            snippet = " ".join(sentences[j] for j in sorted(idx_range)).strip()
            results.append(snippet)

    return "\n\n".join(results) if results else "No target phrases found."
# --- arXiv Search and Extraction ---
def get_arXiv_info(topic: str, num_papers: int = 5) -> str:
    print("[Step 1]: Getting Links from arXiv")
    search_url = (
        f"https://arxiv.org/search/?query={topic}"
        "&searchtype=all&abstracts=show&order=-announced_date_first&size=50"
    )
    resp = requests.get(search_url)
    resp.raise_for_status()
    doc = BeautifulSoup(resp.text, "html.parser")

    links = []
    for a in doc.find_all('a', href=True):
        href = a['href']
        if href.startswith('https://arxiv.org/abs/') and is_new_link(href):
            links.append(href)
            if len(links) >= num_papers:
                break
    links = list(dict.fromkeys(links))[:num_papers]

    abstracts = []
    for i, link in enumerate(links, start=1):
        print(f"[Paper {i}] Fetching abstract and PDF content from {link}")
        paper_resp = requests.get(link)
        paper_resp.raise_for_status()
        paper_soup = BeautifulSoup(paper_resp.text, "html.parser")

        meta_tag = paper_soup.find("meta", {"property": "og:description"})
        abstract = meta_tag["content"] if meta_tag and meta_tag.get("content") else "No abstract found."
        target_phrases = ["space group", "electronic properties", "elastic properties", "bulk modulus", "Young modulus", "Shear modulus", "Poissonâ€™s ratio", "tangent modulus", "chord modulus", "hardness", "electrical conductivity", "carrier concentration", "carrier mobility", "band gap", "electron effective mass", "dielectric constant" ,"permittivity" , "charge recombination and lifetime"]
        # PDF text extraction
        try:
            pdf_text = download_pdf_text(link)
            result_snippet = extract_results_section(pdf_text)
            properties = extract_neighbor_sentences(pdf_text, target_phrases)
        except Exception as e:
            result_snippet = f"Error reading PDF: {e}"
            properties = "N/A"

        abstracts.append(
            f"Publication #{i} ({link}):\n"
            f"Abstract:\n{abstract}\n\n"
            f"PDF Results Section (100 words max):\n{result_snippet}\n\n"
            f"Context Around 'Space Group':\n{properties}\n"
        )

    return "\n\n".join(abstracts)

# --- ChatGPT Answering Function ---
def ask_chatgpt(abstracts: str, topic: str, question: str) -> str:
    print("[Step 3] Sending to ChatGPT...")
    client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
    prompt = (
        f"I have collected the following abstracts and extracted content on '{topic}':\n\n"
        f"{abstracts}\n\n"
        f"Based on this, please answer the following question:\n"
        f"{question}\n\n"
        f"Respond with concise answer in the form of bullet points and cite publication numbers/links. "
        f"If the info is insufficient, say 'More Information Required'."
    )
    response = client.chat.completions.create(
        model="gpt-4.1-nano",
        messages=[
            {"role": "system", "content": "You are an expert assistant summarizing scientific papers."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.5,
        max_tokens=1500
    )
    return response.choices[0].message.content


# --- Main ---

def main():
    topic = name_entry.get()
    abstracts = get_arXiv_info(topic)
    question = question_entry.get()
    answer = ask_chatgpt(abstracts, topic, question)
    print(answer)
    messagebox.showinfo(f"ChatGPT Response:", f"{answer}")



# Create main window
root = tk.Tk()
root.title("Responsive GUI")
root.geometry("400x200")

# Configure grid columns to expand with window
root.columnconfigure(0, weight=1)
root.columnconfigure(1, weight=3)

# --- Topic Label and Entry ---
label1 = tk.Label(root, text="Enter topic:")
label1.grid(row=0, column=0, sticky="e", padx=5, pady=0)
name_entry = tk.Entry(root)
name_entry.grid(row=0, column=1, sticky="ew", padx=0)

# --- Question Label and Entry ---
label2 = tk.Label(root, text="Enter question:")
label2.grid(row=1, column=0, sticky="e", padx=5, pady=0)

question_entry = tk.Entry(root)
question_entry.grid(row=1, column=1, sticky="ew", padx=0)

# --- Submit Button ---
submit_button = tk.Button(root, text="Submit", command=main)
submit_button.grid(row=2, column=0, columnspan=2, pady=20)


# Allow resizing
root.grid_rowconfigure(0, weight=1)
root.grid_rowconfigure(1, weight=1)

root.mainloop()


